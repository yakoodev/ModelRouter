# ModelRouter / MultiLlm

–†—É—Å—Å–∫–æ—è–∑—ã—á–Ω–∞—è –æ—Å–Ω–æ–≤–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –ø—Ä–æ–µ–∫—Ç–∞.  
English version: `README.en.md`

`ModelRouter` (`MultiLlm`) ‚Äî .NET 10 –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ —Å –µ–¥–∏–Ω—ã–º –∫–æ–Ω—Ç—Ä–∞–∫—Ç–æ–º –¥–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö LLM-–ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤:
- OpenAI-compatible endpoints (–≤–∫–ª—é—á–∞—è –ª–æ–∫–∞–ª—å–Ω—ã–µ –∏ self-hosted),
- Ollama —á–µ—Ä–µ–∑ OpenAI-compatible –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å,
- Codex (dev-only, —á–µ—Ä–µ–∑ `codex login`/`auth.json`),
- MCP tools –∫–ª–∏–µ–Ω—Ç –ø–æ stdio.

## –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ
- [1. –¢–µ–∫—É—â–∏–π —Å—Ç–∞—Ç—É—Å](#1-—Ç–µ–∫—É—â–∏–π-—Å—Ç–∞—Ç—É—Å)
- [2. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞](#2-–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞)
- [3. –°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è](#3-—Å—Ç—Ä—É–∫—Ç—É—Ä–∞-—Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è)
- [4. –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç](#4-–±—ã—Å—Ç—Ä—ã–π-—Å—Ç–∞—Ä—Ç)
- [5. –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è API](#5-–ø—Ä–∏–º–µ—Ä-–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è-api)
- [6. –ü—Ä–æ–≤–∞–π–¥–µ—Ä—ã –∏ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è](#6-–ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã-–∏-–∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è)
- [7. –ö–æ–Ω—Ç—Ä–∞–∫—Ç—ã Core](#7-–∫–æ–Ω—Ç—Ä–∞–∫—Ç—ã-core)
- [8. –£—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∏ hooks](#8-—É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å-–∏-hooks)
- [9. MCP](#9-mcp)
- [10. –ü—Ä–∏–º–µ—Ä—ã –∑–∞–ø—É—Å–∫–∞](#10-–ø—Ä–∏–º–µ—Ä—ã-–∑–∞–ø—É—Å–∫–∞)
- [11. –¢–µ—Å—Ç—ã](#11-—Ç–µ—Å—Ç—ã)
- [12. –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è](#12-–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è)
- [13. Git/Codex push setup](#13-gitcodex-push-setup)

## 1. –¢–µ–∫—É—â–∏–π —Å—Ç–∞—Ç—É—Å

### –†–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ
- `MultiLlm.Core`:
  - –µ–¥–∏–Ω—ã–µ –∫–æ–Ω—Ç—Ä–∞–∫—Ç—ã (`ChatRequest`, `ChatResponse`, `ChatDelta`, `Message`, `MessagePart`),
  - —Ä–æ—É—Ç–∏–Ω–≥ –º–æ–¥–µ–ª–∏ —Ñ–æ—Ä–º–∞—Ç–∞ `providerId/model`,
  - resilience pipeline –≤ `LlmClient` (retry/backoff/timeout/concurrency/rate delay),
  - instruction layers (`system`, `developer`, `session`, `request`),
  - hooks (`OnStart`, `OnEnd`, `OnError`) + —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–µ–∫—Ä–µ—Ç–æ–≤.
- `MultiLlm.Providers.OpenAICompatible`:
  - sync chat (`/chat/completions`),
  - streaming SSE (`data: ...`, `[DONE]`),
  - –ø–æ–¥–¥–µ—Ä–∂–∫–∞ `TextPart`, `ImagePart`, `FilePart`, tool parts.
- `MultiLlm.Providers.Codex`:
  - dev-only guard,
  - auth backend slot (`official-device-code` + optional experimental),
  - ChatGPT backend adapter.
- `MultiLlm.Providers.Ollama`:
  - `OllamaOpenAiCompatProvider` (–æ–±–µ—Ä—Ç–∫–∞ –Ω–∞–¥ OpenAI-compatible).
- `MultiLlm.Tools.Mcp`:
  - stdio JSON-RPC client (`initialize`, `tools/list`, `tools/call`).
- –ü—Ä–∏–º–µ—Ä—ã:
  - `examples/ConsoleChat`,
  - `examples/McpDemo`.

### –ù–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ
- `OpenAiProvider` –ø–æ–∫–∞ `NotImplementedException`.
- `OllamaNativeProvider` –ø–æ–∫–∞ `NotImplementedException`.
- `MultiLlm.Extras.ImageProcessing` —Å–æ–¥–µ—Ä–∂–∏—Ç –∫–æ–Ω—Ç—Ä–∞–∫—Ç, –±–µ–∑ –ø–æ–ª–Ω–æ–≥–æ pipeline.

## 2. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞

–ö–ª–∏–µ–Ω—Ç—Å–∫–∏–π –∫–æ–¥ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å `ILlmClient`, –∞ –ø—Ä–æ–≤–∞–π–¥–µ—Ä –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –ø–æ `ChatRequest.Model`:
- `openai-compatible/gpt-4.1-mini`
- `codex/gpt-5-codex`
- `ollama-openai-compat/llama3.1:8b`

`LlmClient`:
- –≤—ã–¥–µ–ª—è–µ—Ç `providerId` –∏–∑ `providerId/model`,
- –≤—ã–±–∏—Ä–∞–µ—Ç `IModelProvider`,
- –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ (`InstructionNormalizer`),
- –ø—Ä–∏–º–µ–Ω—è–µ—Ç resilience-–Ω–∞—Å—Ç—Ä–æ–π–∫–∏,
- –≤—ã—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ (`ProviderId`, `RequestId`, `CorrelationId`).

## 3. –°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è

```text
/src
  /MultiLlm.Core
  /MultiLlm.Providers.OpenAI
  /MultiLlm.Providers.OpenAICompatible
  /MultiLlm.Providers.Ollama
  /MultiLlm.Providers.Codex
  /MultiLlm.Tools.Mcp
  /MultiLlm.Extras.ImageProcessing
/examples
  /ConsoleChat
  /McpDemo
/tests
  /MultiLlm.Core.Tests
  /MultiLlm.Integration.Tests
/docs
  codex-web-playbook.md
  codex-git-setup.md
```

## 4. –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç

–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:
- .NET SDK —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π `net10.0`.

–°–±–æ—Ä–∫–∞:

```bash
dotnet build MultiLlm.slnx
```

–¢–µ—Å—Ç—ã:

```bash
dotnet test MultiLlm.slnx
```

## 5. –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è API

```csharp
using MultiLlm.Core.Abstractions;
using MultiLlm.Core.Contracts;
using MultiLlm.Providers.OpenAICompatible;

var provider = new OpenAiCompatibleProvider(new OpenAiCompatibleProviderOptions
{
    ProviderId = "openai-compatible",
    BaseUrl = "https://api.openai.com/v1",
    Model = "gpt-4.1-mini",
    Headers = new Dictionary<string, string>
    {
        ["Authorization"] = $"Bearer {Environment.GetEnvironmentVariable("OPENAI_API_KEY")}"
    }
});

var client = new LlmClient([provider]);

var request = new ChatRequest(
    Model: "openai-compatible/gpt-4.1-mini",
    Messages:
    [
        new Message(MessageRole.User, [new TextPart("–°–¥–µ–ª–∞–π –∫—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ...")])
    ]);

var response = await client.ChatAsync(request);
var text = string.Concat(response.Message.Parts.OfType<TextPart>().Select(x => x.Text));
Console.WriteLine(text);
```

–î–ª—è stream-—Ä–µ–∂–∏–º–∞ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ `await foreach` –ø–æ `client.ChatStreamAsync(request)`.

## 6. –ü—Ä–æ–≤–∞–π–¥–µ—Ä—ã –∏ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è

### OpenAI-compatible
- –†–µ–∞–ª—å–Ω—ã–π HTTP-–ø—Ä–æ–≤–∞–π–¥–µ—Ä –∫ `POST {baseUrl}/chat/completions`.
- –ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —á–µ—Ä–µ–∑ `Headers` (–Ω–∞–ø—Ä–∏–º–µ—Ä, `Authorization: Bearer ...`).

### Codex (dev-only)
- –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏—Å–ø–æ–ª—å–∑—É–µ—Ç `OfficialDeviceCodeBackend`, –∫–æ—Ç–æ—Ä—ã–π —á–∏—Ç–∞–µ—Ç:
  - `OPENAI_API_KEY`, –ª–∏–±–æ
  - `tokens.access_token` –∏–∑ `CODEX_HOME/auth.json` (–∏–ª–∏ `~/.codex/auth.json`).
- –ü—Ä–∏ `IsDevelopment = false` –≤—ã–±—Ä–∞—Å—ã–≤–∞–µ—Ç—Å—è –∏—Å–∫–ª—é—á–µ–Ω–∏–µ.
- `EnableExperimentalAuthAdapters` –≤–∫–ª—é—á–∞–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π backend-—Å–ª–æ—Ç (–µ—Å–ª–∏ backend –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω).

### Ollama
- `OllamaOpenAiCompatProvider` —Ä–∞–±–æ—Ç–∞–µ—Ç —á–µ—Ä–µ–∑ —Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π OpenAI endpoint.
- `OllamaNativeProvider` –ø–æ–∫–∞ –Ω–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω.

### OpenAI
- `OpenAiProvider` –ø–æ–∫–∞ –Ω–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω.

### Auth-—Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –≤ Core
- `NoAuth`
- `ApiKeyAuth`
- `BearerAuth`
- `CustomHeadersAuth`
- `ITokenStore` + `InMemoryTokenStore`

## 7. –ö–æ–Ω—Ç—Ä–∞–∫—Ç—ã Core

### –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å—ã
- `ILlmClient`
  - `Task<ChatResponse> ChatAsync(ChatRequest, CancellationToken)`
  - `IAsyncEnumerable<ChatDelta> ChatStreamAsync(ChatRequest, CancellationToken)`
- `IModelProvider`
  - `ProviderId`
  - `Capabilities`
  - `ChatAsync(...)`
  - `ChatStreamAsync(...)`

### Chat-–º–æ–¥–µ–ª–∏
- `ChatRequest`:
  - `Model` (`providerId/model`),
  - `Messages`,
  - `Instructions`,
  - `RequestId`, `CorrelationId` (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ).
- `Message`:
  - `Role`: `System`, `Developer`, `User`, `Assistant`, `Tool`,
  - `Parts`: –º–∞—Å—Å–∏–≤ `MessagePart`.
- –¢–∏–ø—ã `MessagePart`:
  - `TextPart`
  - `ImagePart`
  - `FilePart`
  - `ToolCallPart`
  - `ToolResultPart`

### –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
- –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è 4 —Å–ª–æ—è: `system`, `developer`, `session`, `request`.
- –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: `request > session > developer > system`.

## 8. –£—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∏ hooks

`LlmClientResilienceOptions`:
- `MaxRetries` (default `2`)
- `InitialBackoff` / `MaxBackoff` / `UseJitter`
- `RequestTimeout`
- `MaxConcurrentRequests`
- `MinDelayBetweenRequests`
- `ShouldRetry`

`ILlmEventHook`:
- `OnStartAsync`
- `OnEndAsync`
- `OnErrorAsync`

–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å:
- `SecretRedactor` –º–∞—Å–∫–∏—Ä—É–µ—Ç bearer/api key/token/secret –≤ —Å–æ–æ–±—â–µ–Ω–∏—è—Ö –æ—à–∏–±–æ–∫.

## 9. MCP

`MultiLlm.Tools.Mcp` —Å–æ–¥–µ—Ä–∂–∏—Ç `StdioMcpClient`:
- –∑–∞–ø—É—Å–∫–∞–µ—Ç MCP server subprocess (`Command` + `Arguments`),
- –¥–µ–ª–∞–µ—Ç handshake (`initialize` + `notifications/initialized`),
- –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç:
  - `GetToolsAsync()`
  - `CallToolAsync(toolName, argumentsJson)`

–û–ø—Ü–∏–∏:
- `Command`, `Arguments`, `WorkingDirectory`
- `EnvironmentVariables`
- `RequestTimeout`
- `ClientName`, `ClientVersion`, `ProtocolVersion`

## 10. –ü—Ä–∏–º–µ—Ä—ã –∑–∞–ø—É—Å–∫–∞

»ÌÚÂ‡ÍÚË‚Ì˚È Á‡ÔÛÒÍ (Ì‡ÒÚÓÈÍ‡ ÓÛÚ‡ ‚ ÔËÎÓÊÂÌËË):
```bash
dotnet run --project examples/ConsoleChat
```

Codex (ÔˇÏÓÈ Á‡ÔÛÒÍ):
```bash
dotnet run --project examples/ConsoleChat -- --model gpt-5-codex --auth codex
```

API key:
```bash
dotnet run --project examples/ConsoleChat -- --model gpt-5-mini --auth apikey --api-key <KEY>
```

–õ–æ–∫–∞–ª—å–Ω—ã–π endpoint –±–µ–∑ auth:
```bash
dotnet run --project examples/ConsoleChat -- --model llama3.1:8b --auth none --base-url http://localhost:11434/v1
```

MCP demo:
```bash
dotnet run --project examples/McpDemo
```

## 11. –¢–µ—Å—Ç—ã

`MultiLlm.Core.Tests` –ø–æ–∫—Ä—ã–≤–∞–µ—Ç:
- —Ä–æ—É—Ç–∏–Ω–≥ `LlmClient`,
- instruction layers,
- resilience pipeline,
- auth strategies,
- token store,
- Codex auth/backend slot,
- OpenAI-compatible mapper.

`MultiLlm.Integration.Tests` –ø–æ–∫—Ä—ã–≤–∞–µ—Ç:
- OpenAI-compatible provider (sync + stream),
- –∫—Ä–æ—Å—Å-–ø—Ä–æ–≤–∞–π–¥–µ—Ä–Ω—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏,
- MCP client end-to-end —Å mock server.

## 12. –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è

- OpenAI official provider –∏ Ollama native provider –Ω–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω—ã.
- Image processing extras –ø–æ–∫–∞ –Ω–∞ —Ä–∞–Ω–Ω–µ–º —ç—Ç–∞–ø–µ.
- –ù–µ—Ç –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ DI-–ø–∞–∫–µ—Ç–∞ —Å extension methods –¥–ª—è –ø–æ–ª–Ω–æ–π –∞–≤—Ç–æ–∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏.

## 13. Git/Codex push setup

–î–ª—è –≥–ª–æ–±–∞–ª—å–Ω–æ–π –∏ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ push/pull Codex CLI –Ω–∞ Windows —Å–º.:
- `docs/codex-git-setup.md`

–ö—Ä–∏—Ç–∏—á–Ω–æ:
- –Ω–µ –∫–æ–º–º–∏—Ç–∏—Ç—å `.git/.codex-credentials`,
- –µ—Å–ª–∏ PAT –ø–æ–ø–∞–ª –≤ –ª–æ–≥–∏ –∏–ª–∏ —á–∞—Ç, –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ –æ—Ç–æ–∑–≤–∞—Ç—å –∏ —Å–æ–∑–¥–∞—Ç—å –Ω–æ–≤—ã–π.

## –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã

- Technical specification: `MultiLlm_TZ.ModelRouter.md`
- Codex workflow notes: `docs/codex-web-playbook.md`

